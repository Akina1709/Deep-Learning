{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFe05S6GjkqJ"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "Bùi Thị Thanh Xuân\n",
        "19110522\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Giải thích vì sao khi dùng Naive-Learning thì xe taxi chỉ đứng yên một chỗ?**\n",
        "\n",
        "**Trả lời:** Naive-Learning là một quá trình trong đó agent có một mục tiêu mà không cần biết cách đạt được mục tiêu nhưng lại nhận được phần thưởng từ môi trường. Cho nên khi Intialize có thể khiến taxi rơi vào vị trí không di chuyển được, nhưng reward vẫn được nhận cho action này."
      ],
      "metadata": {
        "id": "YH3aUL4qlB8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Giải thích vì sao khi dùng Q-Leaning thì xe taxi có thể đón và trả khách được?**\n",
        "\n",
        "**Trả lời**:  Q-Learning tìm ra các action tối ưu ở mỗi state. Chọn action có giá trị cao cho nó khi mà nó nhìn vô cái Q-table, nếu value là 0 cho tất cả các option, chọn 1 random action, Bắt đầu với 100% là exploration và giảm dần đến 0% nó chuyển sang mode exploitation."
      ],
      "metadata": {
        "id": "lgqgww_PtyW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Tìm hiểu một game khác trên OpenAI và thiết lập cho agent chơi được**\n"
      ],
      "metadata": {
        "id": "8uLOAofsvs1L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solving the CartPole balancing environment**\n",
        "\n",
        "- Ý tưởng của CartPole là có một cái cột đứng trên đầu một chiếc xe đẩy. Mục tiêu là cân bằng cột này bằng cách ngọ nguậy/di chuyển xe đẩy từ bên này sang bên kia để giữ cho cột thăng bằng thẳng đứng.\n",
        "\n",
        "- Môi trường được coi là thành công nếu chúng ta có thể cân bằng trong 200 khung hình và thất bại được coi là khi cột nghiêng hơn 15 độ so với phương thẳng đứng hoàn toàn.\n",
        "\n",
        "- Mỗi khung hình mà chúng tôi thực hiện với cột \"balanced\" (dưới 15 độ so với phương thẳng đứng), \"điểm số\" của chúng tôi được +1 và mục tiêu của chúng tôi là số điểm 200.\n",
        "\n",
        "- Để minh họa điều này, chúng ta sẽ bắt đầu bằng cách tạo một agent, khi ở trong môi trường cartpole, nó chỉ chọn ngẫu nhiên các actions (trái và phải). Hãy nhớ lại rằng mục tiêu của chúng tôi là đạt được số điểm 200, nhưng chúng tôi sẽ tiếp tục và sử dụng bất kỳ tình huống nào mà chúng tôi đã ghi được trên 50 điểm để học hỏi.\n",
        "\n"
      ],
      "metadata": {
        "id": "m3EkVft49M79"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cython"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vE0ETxkw1ZXI",
        "outputId": "c9df9ca5-c595-494b-9c02-25581b585c04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.8/dist-packages (0.29.32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maFy2CId1h2m",
        "outputId": "a5b9a895-86cb-42f2-cfff-b65be43fba14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.8/dist-packages (2.9.2)\n",
            "Requirement already satisfied: keras<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (14.0.6)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: flatbuffers<2,>=1.12 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.12)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: protobuf<3.20,>=3.9.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.19.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from tensorflow) (21.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.10.0,>=2.9.0rc0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.21.6)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.51.1)\n",
            "Requirement already satisfied: tensorboard<2.10,>=2.9 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.9.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (0.28.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.3.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/dist-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (2.15.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/dist-packages (from tensorboard<2.10,>=2.9->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/dist-packages (from markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (5.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.10,>=2.9->tensorflow) (3.11.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard<2.10,>=2.9->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->tensorflow) (3.0.9)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install scipy h5py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nwJL3lmC1owj",
        "outputId": "c5273170-f269-418a-d858-1f1cd27f164f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.8/dist-packages (1.7.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.8/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy<1.23.0,>=1.16.5 in /usr/local/lib/python3.8/dist-packages (from scipy) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5joflNbG2C6E",
        "outputId": "06d47f1f-76ab-486c-887e-28c429b6256d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.8/dist-packages (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U --no-deps tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbVmEvCF2F73",
        "outputId": "e47edfd0-05d3-4f9e-e942-200b7a8d9f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tflearn\n",
            "  Downloading tflearn-0.5.0.tar.gz (107 kB)\n",
            "\u001b[K     |████████████████████████████████| 107 kB 14.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: tflearn\n",
            "  Building wheel for tflearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tflearn: filename=tflearn-0.5.0-py3-none-any.whl size=127299 sha256=9571d7d6e82b1e69bb4ac445c0cee0aa4b3d7f8e1c7459069ff2b7a348072683\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/9b/15/cb1e6b279c14ed897530d15cfd7da8e3df8a947e593f5cfe59\n",
            "Successfully built tflearn\n",
            "Installing collected packages: tflearn\n",
            "Successfully installed tflearn-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gym[classic_control]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVGgriqX3GGh",
        "outputId": "69d5689e-e9f6-4f6e-8f20-f9d522673244"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.5.0)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (5.1.0)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 18.3 MB 15.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pygame \n",
        "\n",
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHsClhlj3pK6",
        "outputId": "2cd4bcbf-dad5-4619-9beb-8dc0620b5d4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.8/dist-packages (2.1.0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "import tflearn\n",
        "\n",
        "from tflearn.layers.core import input_data, dropout, fully_connected\n",
        "from tflearn.layers.estimator import regression\n",
        "from statistics import median, mean\n",
        "from collections import Counter\n",
        "\n",
        "LR = 1e-3\n",
        "env = gym.make(\"CartPole-v0\")\n",
        "env.reset()\n",
        "goal_steps = 500\n",
        "score_requirement = 50\n",
        "initial_games = 10000"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dtm0l3x1BFi",
        "outputId": "9fd51928-5434-475f-bf31-57eb3e77d1ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.8/dist-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "/usr/local/lib/python3.8/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.8/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def some_random_games_first():\n",
        "    # Each of these is its own game.\n",
        "    for episode in range(5):\n",
        "        env.reset()\n",
        "        # this is each frame, up to 200...but we wont make it that far.\n",
        "        for t in range(200):\n",
        "            # This will display the environment\n",
        "            # Only display if you really want to see it.\n",
        "            # Takes much longer to display it.\n",
        "            env.render()\n",
        "            \n",
        "            # This will just create a sample action in any environment.\n",
        "            # In this environment, the action can be 0 or 1, which is left or right\n",
        "            action = env.action_space.sample()\n",
        "            \n",
        "            # this executes the environment with an action, \n",
        "            # and returns the observation of the environment, \n",
        "            # the reward, if the env is over, and other info.\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            if done:\n",
        "                break"
      ],
      "metadata": {
        "id": "eZZdx_eI2T5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_population():\n",
        "    # [OBS, MOVES]\n",
        "    training_data = []\n",
        "    # all scores:\n",
        "    scores = []\n",
        "    # just the scores that met our threshold:\n",
        "    accepted_scores = []\n",
        "    # iterate through however many games we want:\n",
        "    for _ in range(initial_games):\n",
        "        score = 0\n",
        "        # moves specifically from this environment:\n",
        "        game_memory = []\n",
        "        # previous observation that we saw\n",
        "        prev_observation = []\n",
        "        # for each frame in 200\n",
        "        for _ in range(goal_steps):\n",
        "            # choose random action (0 or 1)\n",
        "            action = random.randrange(0,2)\n",
        "            # do it!\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            \n",
        "            # notice that the observation is returned FROM the action\n",
        "            # so we'll store the previous observation here, pairing\n",
        "            # the prev observation to the action we'll take.\n",
        "            if len(prev_observation) > 0 :\n",
        "                game_memory.append([prev_observation, action])\n",
        "            prev_observation = observation\n",
        "            score+=reward\n",
        "            if done: break\n",
        "\n",
        "        # IF our score is higher than our threshold, we'd like to save\n",
        "        # every move we made\n",
        "        # NOTE the reinforcement methodology here. \n",
        "        # all we're doing is reinforcing the score, we're not trying \n",
        "        # to influence the machine in any way as to HOW that score is \n",
        "        # reached.\n",
        "        if score >= score_requirement:\n",
        "            accepted_scores.append(score)\n",
        "            for data in game_memory:\n",
        "                # convert to one-hot (this is the output layer for our neural network)\n",
        "                if data[1] == 1:\n",
        "                    output = [0,1]\n",
        "                elif data[1] == 0:\n",
        "                    output = [1,0]\n",
        "                    \n",
        "                # saving our training data\n",
        "                training_data.append([data[0], output])\n",
        "\n",
        "        # reset env to play again\n",
        "        env.reset()\n",
        "        # save overall scores\n",
        "        scores.append(score)\n",
        "    \n",
        "    # just in case you wanted to reference later\n",
        "    training_data_save = np.array(training_data)\n",
        "    np.save('saved.npy',training_data_save)\n",
        "    \n",
        "    # some stats here, to further illustrate the neural network magic!\n",
        "    print('Average accepted score:',mean(accepted_scores))\n",
        "    print('Median score for accepted scores:',median(accepted_scores))\n",
        "    print(Counter(accepted_scores))\n",
        "    \n",
        "    return training_data"
      ],
      "metadata": {
        "id": "VQUDQNJl2YmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network_model(input_size):\n",
        "\n",
        "    network = input_data(shape=[None, input_size, 1], name='input')\n",
        "\n",
        "    network = fully_connected(network, 128, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    network = fully_connected(network, 256, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    network = fully_connected(network, 512, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    network = fully_connected(network, 256, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    network = fully_connected(network, 128, activation='relu')\n",
        "    network = dropout(network, 0.8)\n",
        "\n",
        "    network = fully_connected(network, 2, activation='softmax')\n",
        "    network = regression(network, optimizer='adam', learning_rate=LR, loss='categorical_crossentropy', name='targets')\n",
        "    model = tflearn.DNN(network, tensorboard_dir='log')\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def train_model(training_data, model=False):\n",
        "\n",
        "    X = np.array([i[0] for i in training_data]).reshape(-1,len(training_data[0][0]),1)\n",
        "    y = [i[1] for i in training_data]\n",
        "\n",
        "    if not model:\n",
        "        model = neural_network_model(input_size = len(X[0]))\n",
        "    \n",
        "    model.fit({'input': X}, {'targets': y}, n_epoch=5, snapshot_step=500, show_metric=True, run_id='openai_learning')\n",
        "    return model"
      ],
      "metadata": {
        "id": "6tBtymIM2b5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = initial_population()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHLQ7GA32fRt",
        "outputId": "72d48d90-4058-4de4-c434-90f77fe7f871"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accepted score: 61.05555555555556\n",
            "Median score for accepted scores: 58.0\n",
            "Counter({50.0: 38, 51.0: 27, 54.0: 25, 55.0: 22, 52.0: 20, 53.0: 20, 56.0: 20, 60.0: 17, 58.0: 15, 61.0: 13, 63.0: 13, 64.0: 12, 57.0: 11, 67.0: 10, 62.0: 9, 65.0: 9, 72.0: 9, 59.0: 8, 70.0: 8, 69.0: 6, 73.0: 6, 71.0: 6, 76.0: 5, 68.0: 5, 75.0: 5, 83.0: 4, 66.0: 4, 79.0: 3, 85.0: 3, 74.0: 3, 77.0: 3, 84.0: 2, 80.0: 2, 78.0: 2, 81.0: 1, 99.0: 1, 87.0: 1, 82.0: 1, 102.0: 1, 86.0: 1, 97.0: 1, 101.0: 1, 136.0: 1, 88.0: 1, 95.0: 1, 89.0: 1, 91.0: 1})\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-d18fe9924588>:55: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  training_data_save = np.array(training_data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_model(training_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gU6DQCTt2xCL",
        "outputId": "a9aa24a9-8ad2-4616-f535-f204f7bb61fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Step: 1774  | total loss: \u001b[1m\u001b[32m0.67580\u001b[0m\u001b[0m | time: 2.735s\n",
            "| Adam | epoch: 005 | loss: 0.67580 - acc: 0.5855 -- iter: 22656/22701\n",
            "Training Step: 1775  | total loss: \u001b[1m\u001b[32m0.67540\u001b[0m\u001b[0m | time: 2.741s\n",
            "| Adam | epoch: 005 | loss: 0.67540 - acc: 0.5848 -- iter: 22701/22701\n",
            "--\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = []\n",
        "choices = []\n",
        "for each_game in range(10):\n",
        "    score = 0\n",
        "    game_memory = []\n",
        "    prev_obs = []\n",
        "    env.reset()\n",
        "    for _ in range(goal_steps):\n",
        "        env.render()\n",
        "\n",
        "        if len(prev_obs)==0:\n",
        "            action = random.randrange(0,2)\n",
        "        else:\n",
        "            action = np.argmax(model.predict(prev_obs.reshape(-1,len(prev_obs),1))[0])\n",
        "\n",
        "        choices.append(action)\n",
        "                \n",
        "        new_observation, reward, done, info = env.step(action)\n",
        "        prev_obs = new_observation\n",
        "        game_memory.append([new_observation, action])\n",
        "        score+=reward\n",
        "        if done: break\n",
        "\n",
        "    scores.append(score)\n",
        "\n",
        "print('Average Score:',sum(scores)/len(scores))\n",
        "print('choice 1:{}  choice 0:{}'.format(choices.count(1)/len(choices),choices.count(0)/len(choices)))\n",
        "print(score_requirement)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAOC-zv_2nNJ",
        "outputId": "6365d7dc-25ea-4e5c-80b5-1415f79e6b6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Score: 200.0\n",
            "choice 1:0.4965  choice 0:0.5035\n",
            "50\n"
          ]
        }
      ]
    }
  ]
}